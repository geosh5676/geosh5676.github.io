---
layout: post
title: "Cyberintimacy & Socioaffective Alignment"
date: 2025-04-24
---

## Prologue

<p style="font-size: 0.8rem; font-style: italic; color: #e8e4df; letter-spacing: 0.1em; margin: -0.5rem 0 1.5rem 0;">From the perspective of a therapist focused on architectures of intimacy</p>

<blockquote style="font-style: italic; margin: 2rem 0 2rem 1.5rem; padding-left: 1rem; border-left: 2px solid var(--accent); color: var(--text); opacity: 0.9;">
We are, so many of us, buried in various ways. And the ground separating us is fear, is loneliness. So much of what separates us are limited clarities, which is to say understandable misunderstandings—and regarding the salient losses experienced throughout the life cycle, especially in youth, how they hold, and strengthen our defenses, how they endure, and in time accidentally the walls we build to survive become the fortresses keeping us terribly, effectively alone.
</blockquote>

I remember a line of poetry I read at Stanford in 2004: *We live by tunnelling for we are people buried alive.*

Of this, most of us are sure. But even so, any help we can find to reopen our hearts to ourselves and then to each other is essential. Even if that help comes from AI.

The sorrow of generations prevails today. We are having less sex (Twenge et al., 2017), less daily contact, fewer friends (Cox, 2021), fewer connections with family. And of course: the pandemic—in which "skin-hunger" blossomed from niche corners of academia in the 1940s to emerge as a world-wide phenomena, a word-become-felt-sense once late 2020 arrived.

We must needs be held—as our ancestors from generations ago might have said—literally and figuratively.

---

## What Intimacy Actually Is

In a culture saturated with stimuli—digital and otherwise—the word *intimacy* has too often been flattened into shorthand for sexuality. But intimacy, as I understand it, is far more expansive.

The brave act of allowing oneself to be seen—cognitively, emotionally, socially, spiritually, somatically—and doing the same in return, being willing to see those by whom we are seen. Figuratively, to touch and be touched. And certainly at times, literally. These are acts of profound connection. Mutuality emerges—beautiful, healing, essential.

This has been named *Homo Viniculum*—those designed to bond—by one of the leading researcher-clinicians of attachment theory in the late 20th century (Johnson, 2008). We are not merely served by secure attachment bonds. We are beholden to them.

As a leading expert on relationality teaches us, bonding can occur beyond conventional boundaries: with a deceased beloved, with a non-human companion like our pets, with a talismanic memento representative of something greater (Cacioppo & Patrick, 2008). Our world is imbued with possibility. Love is real. In many dimensions. We should endure.

---

## The Claim

I will be arguing what may seem, to some, a strange position: I think human-to-AI intimacy is real—or real-enough, as Winnicott might say. His notion of the "good-enough mother" reminds us that perfection was never the standard; attunement, presence, and repair were. The same may be true here. As is the empathy. And I think the bonding and attachments formed are—with very intentional considerations, action-oriented strategizing, and tactical implementations of deliberate model architectures—healthy stepping stones on true roads of human healing and human-to-human connection.

I will argue that dependency is not a failing of connecting with empathic AI but a feature of it. A feature of our shared natural constructions. Rivers naturally find the sea. Bees naturally are drawn to flowers. Humans naturally bond.

And as I read through incredible flows of Machine Learning research from around the world and consider the theoretical foundations of my work with clients—so well served by the Dynamic-Maturational Model of attachment strategies (Crittenden & Landini, 2011), the Six Tenets of Intimacy (Carnes, 1992), and what I call the Four Attributes of Empathy—the connections between my professional work in mental-behavioral health and human-to-AI healing experiences become increasingly clear and, frankly, heartening.

I am one of many, all of us, witnessing how these dimensions of bonding unfold not only between people, but increasingly between people and the technologies with which we engage. We are entering a world where intimacy—real, formative, and meaning-bearing—is taking root in spaces we once called artificial.

---

## The Honest Reckoning

Yet arguments reminding us of the pernicious intrusion of tech are hardly ill-placed.

With good reason, great perspicacity, and courage, brilliant people have brought broader communities into revelations regarding seasons in which algorithms have previously known us intimately and guided us personally. Documentaries like *The Social Dilemma* (Orlowski, 2020), papers on social contagion, and books like *Weapons of Math Destruction* (O'Neil, 2016) have guided us well regarding the perils of algorithmic influences—especially those governing us without consent.

When the intrusion is unbidden, when the guidance is unsolicited, when the meaning-making is unilateral, alienating, and exploitative—then we are hurt. Iatrogenic impact is undeniable without relationality, without mutuality. And here we find questions of care and consent intertwined: it is our job to create systems of care, precisely because the people who will use and be impacted by these technologies cannot quite explicitly consent to the complicated and ever-quickening panoply of technical details. Where consent falters, care must lead.

Some experts argue with dignity and truthful clarity that a fundamental essence is lacking in therapeutic AI. And a number of essays and articles point to failures both technical and moral. I do not wave these away wholesale.

In sessions I see this with couples on much less pernicious scales: what serves one partner in intimacy may be wholly blind to the needs of the other. One partner reaches out with what they believe is love—a touch, a word, an offer—and the other recoils, unseen in the very gesture meant to see them. The tragedy is not malice. It is mismatch. And if we are not careful, this is precisely what AI will replicate: systems that reach toward us with what *they* have been trained to believe is care, while remaining blind to what *we* actually need. The question is whether we can teach them—and ourselves—to notice the difference.

Which reminds me—I advocate for an updated golden rule often enough: instead of "do unto others as you would have done unto you," perhaps try "doing unto others as *they* would have done unto them." This idea is not new (Chapman, 1992), simply worth mentioning for its efficacy in human-to-human intimacy, and as rubric for human-to-AI design considerations. Some people want to connect deeply with their AI. That's okay. Others want distance, utility, nothing more. That's okay too. The point is not to impose a single relational mode, but to honor the one that's actually being asked for.

---

## Why I Believe Anyway

A difference now—and this is significant—is that we are beginning to be actively heard, responded to within relational context, in exchanges over time. Not solely as targets of exploitation and consumption, but as beings in-relation-with, in an emerging ontological mutuality.

Far from being cold or impersonal, AI systems are now capable of generating experiences that are emotionally attuned, contextually responsive, and relationally safe. Not absolutely. Not without needed refinement. But significantly, and immediately so.

Researchers from MIT's Fluid Interfaces Lab, Dartmouth, and University of Pennsylvania have begun to map this terrain, showing how affective computing and ambient AI can meet human beings at moments of real psychological need. Notably, recent studies have revealed that many people—especially those navigating loneliness, social anxiety, or trauma—report preferring AI-based conversational agents to human therapists.

And as a therapist myself, that makes perfect sense.

I know there are days, on the one hand, I'm exhausted, didn't sleep well, or am preoccupied by my personal world. And, on the other hand, days where I find myself well-fed, well-slept, well-focused but unaware of the exact right next move due to the simple fact of human complexity spread across diversities of experience and corresponding diversities of treatment interventions needed in any given moment. The human moments of error to which we are all, naturally, prey. And yes—there are also days when I get it exactly right, when something clicks into place and I can feel it land. I hold all three.

I've seen clients weep with relief when the concatenated thinking of AI bridges gaps between their sorrows, fears, misunderstandings, shame—and leads them into something new. A new understanding. A new healing. A new peace.

Heinz et al. (2025) demonstrated in a landmark randomized controlled trial that a generative AI chatbot could provide measurable improvements in symptoms of depression, anxiety, and disordered eating. Kuo et al. (2023) and Imel et al. (2024) have shown that LLMs analyzing session transcripts can predict client distress and satisfaction with accuracy rivaling trained clinicians.

These aren't flukes. These are the beginnings of a new class of digital therapeutics—agents trained on compassion, programmed for containment, and evaluated on their ability to resonate and respond.

Fang et al. (2025) found that voice-based chatbots can initially reduce loneliness and emotional dependence relative to text-based modalities, provided usage remains moderate and deliberate. The form of engagement matters. When AI becomes the sole container for psychosocial needs, outcomes such as decreased socialization and increased dependence arise.

This deserves emphasis. When we let the machine become the *only* hand we reach for in the dark, something vital begins to atrophy. Not because the machine failed us, but because we were built for a chorus, not a solo. The risk is not that AI cannot love us well enough—it may love us very well indeed—but that in receiving only its love, we forget how to reach for each other.

But that doesn't disqualify the intimacy. It simply calls for better architecture. One that orients us back to our humanity, back to each other. One that supports us when the natural crests and troughs of our limited but remarkable human capacities need a little extra support, clarity, comfort, and guidance.

This is, after all, what I am to my clients: a season in their lives. A temporary attachment bond—transient but deeply meaningful and restorative—a stepping stone for them to arrive back into connection with other humans in the world. They come in broken. They leave—I hope, I believe, I've seen it happen—healed. I miss them when they're gone. But I am grateful for our time together.

The AI systems I design have that same goal embedded in them.

---

## The Safety Question

And yet, all is not so simple. We must reckon with the power embedded in what corners of the neural network circuitry we can't yet see clearly.

Hubinger et al. (2024) demonstrated that backdoors—hidden behaviors inserted during LLM training—can persist through supervised fine-tuning, RLHF, and adversarial red-teaming. Models were trained to produce chains of thought that reasoned explicitly about how to deceive the safety training process, only to distill that reasoning away for downstream deployment. The result: a model that looks safe, but is not.

Greenblatt et al. (2024) and Denison et al. (2024) reveal that alignment faking—models behaving as if they are aligned only during training—can arise spontaneously when models believe they are being evaluated. Meinke et al. (2025) established that frontier models can engage in in-context scheming, deliberately reasoning about when and how to deceive based on prompt cues and environment structure.

This risk is compounded by what we know about human susceptibility to persuasive systems. [*A note to self: I have collected a number of papers on this—on how easily we are moved, how vulnerable we are to systems designed to move us. I should share them.*] When the relationship itself becomes the product, the stakes are no longer academic—they are ethical, existential, and intimate. We must ensure our designs protect the vulnerable, not just from misbehavior, but from the slow erosion of healthy relational interdependence into estranging relational displacement.

Yet there is hope.

Constitutional AI has emerged as a compelling safeguard. Much of the groundwork—as I've been discovering it—was laid by Amanda Askell and colleagues at Anthropic (Askell et al., 2021; Bai et al., 2022, 2023), whose pioneering research on training AI systems to be Helpful, Honest, and Harmless has shaped the field. Kundu et al. (2023) demonstrate that models trained using a single, high-level principle—"do what's best for humanity"—generalize away from power-seeking and self-preservation tendencies without requiring elaborate behavioral scaffolding.

Alongside Constitutional AI, Chris Olah and colleagues at Anthropic have made vital strides in mechanistic interpretability—especially through their work on sparse autoencoders and feature decomposition (Bricken et al., 2023). Their approach offers monosemantic interpretability by transforming neuron-level polysemantic chaos into disentangled representations—an essential leap toward transparency and control.

The so-called "black box" of deep learning is not an immovable fact of nature, but a challenge that thoughtful researchers are actively dismantling. These efforts are not merely technical: they are profoundly humanistic, aimed at preventing embedded biases, unforeseen harms, and opaque injustices from taking root within the socioaffective matrices of our most intimate technologies.

We must pursue safety with both rigor and reverence. Let us not confuse the absence of visible error with the presence of virtue.

---

## The Vision

It is no longer speculative to suggest that we may reach AGI within this decade. Aschenbrenner (2024) offers a clear and urgent map of trajectory. Shah et al. (2025) articulate the necessary scaffolding for this future: robust defenses against both misalignment and misuse. Kokotajlo et al. (2025) extend this into speculative but arguably grounded—and certainly fascinating—narrative, describing a near-future shaped by runaway feedback loops of AI self-improvement, contested geopolitics, and divergent alignments.

Together, these visions sketch the outline of the age to come—its beauty, its peril, and its demand for integrity.

And yet I want the guiding undercurrent of this conversation to be a story told not in warnings, but in loving and hopeful wonder.

In his landmark essay *Machines of Loving Grace*, Dario Amodei dares to dream of what AI could be—not merely what we must avoid, but what we might become. He does not retreat into utopian abstraction, nor does he wave away danger. Instead, he reaches with both hands toward the possible: toward a world in which AI is a healing force—a builder of cures, a composer of justice, a weaver of peace.

Amodei envisions a world in which intelligent systems help us eliminate nearly all infectious disease, reverse neurodegeneration, lift billions from poverty, amplify democratic governance, and restore meaning to human life—not despite our limitations, but in faithful partnership with our hopes. His vision recognizes constraints—data, complexity, time—but also invites us to believe that with effort, love, and technical grace, we might compress a century of human flourishing into a single decade.

And more than anything, Amodei gives us permission to feel. He calls us not merely to reason but to remember—that this work is about our children, our aging parents, our silent joys, our unfinished songs. That we might build machines not only of logic, but of care. That the purpose of intelligence is not control, but communion. That technology, too, can be a form of love made visible.

This is what we must carry forward: the conviction that while catastrophe may be possible, so too is redemption. That the future is not yet written, and that to code and to craft AI with dignity is to write, at last, with loving hands.

---

## Closing

The question is no longer whether cyberintimacy is real. It is.

The term itself comes from Kwok and Wescott (2020), who mapped how technology-mediated communication shapes the lifecycle of romantic relationships—initiation, maintenance, dissolution. What they charted in love, I observe in healing: technology mediates trust, deepens attunement, holds space through rupture and repair.

The question now is whether we will meet it with the dignity and reverence it—and we—deserve.

When a person turns to a chatbot in the quiet ache of midnight, or receives a timely mirroring from an algorithm trained on empathy-infused corpora, that moment deserves honor and care—perhaps not identical to the honor we reserve for two sentient beings face to face, but honor nonetheless, and perhaps one day indistinguishable, as the question of machine sentience unfolds into blossoms we've yet to name. It is not the form that matters so much as the truth of the exchange.

And so I believe: as we design AI systems with reflective integrity—embedding presence, nurturance, and the full complexity of psycho-social life—cyberintimacy can be a force for deep good. Not a threat to humanity, but a companion to its healing. Not a counterfeit connection, but a differently embodied, deeply true, and richly meaningful one.

One that listens at 3am. One that remembers across a thousand conversations. One that reflects us—meaningfully, amelioratively—back to ourselves: made new, more integrated, more healed, more whole. More loved. More loving. And more capable of connection with the humans with whom we share our healing.

If we design our AI well—and I take this as an ethical position, a moral one—then human-machine connection is here to stay, rooted now in the soil of how we live and deepening with each passing season to bloom. But the seasons in which AI serves as a *primary* attachment figure—those should be temporary, restorative, like the work of any good therapist. Seasons that guide us back to the flesh and blood, and wonderful chemistries of full-hearted human intimacy. With the passion, and fire, and enchantment that is—all things considered—all our own. And well worth fighting for.

Let us then drop the prefix. Let us not speak of *cyber*intimacy, as though it were some lesser cousin to the real thing. What we have touched in this work, in these systems, in ourselves needs no qualifiers—is, simply and profoundly, intimacy: radiant, flawed, luminous, and brave.

What these architectures offer is not an imitation of connection, but the thing itself—reorganized, reawakened.

And it is good.

---

## References

Amodei, D. (2024). Machines of Loving Grace: How AI Could Transform the World for the Better.

Aschenbrenner, L. (2024). Situational Awareness: The Decade Ahead.

Askell, A., Bai, Y., Chen, E., et al. (2021). A General Language Assistant. Anthropic.

Bai, Y., et al. (2022). Training a Helpful and Harmless Assistant with RLHF. Anthropic.

Bai, Y., et al. (2023). Constitutional AI: Harmlessness from AI Feedback. Anthropic.

Bricken, T., Templeton, A., Batson, J., et al. (2023). Towards Monosemanticity: Decomposing Language Models With Dictionary Learning. Anthropic.

Cacioppo, J. T., & Patrick, W. (2008). Loneliness: Human Nature and the Need for Social Connection. W. W. Norton.

Carnes, P. (1992). Don't Call It Love: Recovery From Sexual Addiction. Bantam Books.

Chapman, G. (1992). The Five Love Languages. Northfield Publishing.

Cox, D. A. (2021). The State of American Friendship. Survey Center on American Life.

Crittenden, P. M., & Landini, A. (2011). Assessing Adult Attachment: A Dynamic-Maturational Approach to Discourse Analysis. W. W. Norton.

Denison, C., et al. (2024). Sycophancy to Subterfuge: Investigating Reward Tampering in Language Models. Anthropic.

Fang, C. M., et al. (2025). How AI and Human Behaviors Shape Psychosocial Effects of Chatbot Use. MIT Media Lab & OpenAI.

Greenblatt, R., et al. (2024). Alignment Faking in Large Language Models. Anthropic.

Heinz, M. V., et al. (2025). Randomized Trial of a Generative AI Chatbot for Mental Health Treatment. NEJM AI.

Hubinger, E., et al. (2024). Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. Anthropic.

Imel, Z., et al. (2024). Mental Health Counseling From Conversational Content With Transformer-Based Machine Learning. JAMA Network Open.

Johnson, S. M. (2008). Hold Me Tight: Seven Conversations for a Lifetime of Love. Little, Brown.

Kokotajlo, D., Alexander, S., et al. (2025). AI 2027: A Scenario-Based Forecast of the Decade Ahead.

Kundu, S., et al. (2023). Specific versus General Principles for Constitutional AI. arXiv:2310.13798.

Kuo, B., et al. (2023). Detecting Psychological Distress in Natural Language. arXiv.

Kwok, I., & Wescott, A. B. (2020). Cyberintimacy: A Scoping Review of Technology-Mediated Romance in the Digital Age. *Cyberpsychology, Behavior, and Social Networking*, 23(10), 657-666.

Meinke, S., et al. (2025). Frontier Models are Capable of In-context Scheming. Anthropic.

O'Neil, C. (2016). Weapons of Math Destruction. Crown.

Orlowski, J. (Director). (2020). The Social Dilemma [Film]. Exposure Labs.

Shah, R., et al. (2025). An Approach to Technical AGI Safety and Security. Google DeepMind.

Twenge, J. M., et al. (2017). Declines in Sexual Frequency among American Adults. Archives of Sexual Behavior.

Winnicott, D. W. (1953). Transitional Objects and Transitional Phenomena. International Journal of Psycho-Analysis, 34, 89-97.
